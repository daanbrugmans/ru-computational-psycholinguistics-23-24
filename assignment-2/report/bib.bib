@article{fitz2019erp,
    title = {Language ERPs reflect learning through prediction error propagation},
    journal = {Cognitive Psychology},
    volume = {111},
    pages = {15-52},
    year = {2019},
    issn = {0010-0285},
    doi = {https://doi.org/10.1016/j.cogpsych.2019.03.002},
    url = {https://www.sciencedirect.com/science/article/pii/S0010028518300124},
    author = {Hartmut Fitz and Franklin Chang}
}

@misc{frank2024gradients,
 title={Neural language model gradients predict event-related brain potentials},
 url={osf.io/preprints/psyarxiv/cx3h6},
 DOI={10.31234/osf.io/cx3h6},
 publisher={PsyArXiv},
 author={Frank, Stefan L},
 year={2024},
 month={Jan}
}

@article{frank2015erp,
    title = {The ERP response to the amount of information conveyed by words in sentences},
    journal = {Brain and Language},
    volume = {140},
    pages = {1-11},
    year = {2015},
    issn = {0093-934X},
    doi = {https://doi.org/10.1016/j.bandl.2014.10.006},
    url = {https://www.sciencedirect.com/science/article/pii/S0093934X14001515},
    author = {Stefan L. Frank and Leun J. Otten and Giulia Galli and Gabriella Vigliocco}
}

@article{oh2023why,
    author = {Oh, Byung-Doh and Schuler, William},
    title = "{Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {336-350},
    year = {2023},
    month = {03},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00548},
    url = {https://doi.org/10.1162/tacl\_a\_00548},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00548/2075940/tacl\_a\_00548.pdf},
}